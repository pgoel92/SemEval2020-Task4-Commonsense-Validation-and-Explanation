{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x_train = pd.read_csv('data/train/subtaskA_data_all.csv')\n",
    "y_train = pd.read_csv('data/train/subtaskA_answers_all.csv', header=None)\n",
    "x_test = pd.read_csv('data/trial/taskA_trial_data.csv')\n",
    "y_test = pd.read_csv('data/trial/taskA_trial_answer.csv', header=None)\n",
    "y_train = y_train.rename(columns={0 : 'id',1: 'invalid_sent'})\n",
    "y_test = y_test.rename(columns={0 : 'id',1: 'invalid_sent'})\n",
    "xy = x_train.merge(y_train)\n",
    "items = []\n",
    "for row in xy.itertuples():\n",
    "    if row.invalid_sent == 0:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 0.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 1.0})\n",
    "    else:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 1.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 0.0})\n",
    "df = pd.DataFrame(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xytest = x_test.merge(y_test)\n",
    "items = []\n",
    "for row in xytest.itertuples():\n",
    "    if row.invalid_sent == 0:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 0.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 1.0})\n",
    "    else:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 1.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 0.0})\n",
    "testdf = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Logistic Regression #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/goelprat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/goelprat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def preprocess(x):\n",
    "    x = x.lower()\n",
    "    tokens = word_tokenize(x)\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words] \n",
    "    return ' '.join([ps.stem(t) for t in filtered_tokens])\n",
    "pdf = df.sent.apply(preprocess)\n",
    "testpdf = testdf.sent.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False, max_features=3000)\n",
    "vectorizer.fit(pdf)\n",
    "#vectorizer2 = CountVectorizer(min_df=0, lowercase=False, max_features=3000)\n",
    "#vectorizer2.fit(testpdf)\n",
    "len(vectorizer1.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(pdf)\n",
    "X_test = vectorizer.transform(testpdf)\n",
    "Y_train = df.isvalid\n",
    "Y_test = testdf.isvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goelprat/miniconda3/envs/fastai/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774369124195943"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = classifier.score(X_test, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Answer 5\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "### Answer 3\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model(optimizer='sgd'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', input_dim = 3000))\n",
    "    # Describe the loss and how it is optimized\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def grid_search():\n",
    "    optimizers = []\n",
    "    model = KerasClassifier(build_fn=create_model)\n",
    "    optimizer = ['SGD', 'Adam', 'RMSProp']\n",
    "    param_grid = dict(epochs=[20, 40, 60], batch_size=[20, 40, 60], optimizer=optimizer)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        \n",
    "model = create_model(optimizer='adam')\n",
    "model.fit(X_train, Y_train, epochs=40, batch_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4042/4042 [==============================] - 0s 57us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24135216026539735, 0.5821375846862793]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Playing with fastai library #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4042, 1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "class CommonSense_NN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        #self.emb = nn.Embedding(vocab_size, 300) \n",
    "        self.lin1 = nn.Linear(vocab_size, 300, bias=True)\n",
    "        self.lin2 = nn.Linear(300, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = xb.view(-1,784) \n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        #x = self.emb(x)\n",
    "        x = F.relu(self.lin1(x.float()))\n",
    "        x = torch.sigmoid(self.lin2(x))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "class CommonSense_Logistic(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(vocab_size, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        x = F.sigmoid(self.lin1(x.float()))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.data import TokenizeProcessor, NumericalizeProcessor\n",
    "class BoWProcessor(NumericalizeProcessor):\n",
    "    max_vocab = 2000\n",
    "    min_freq=3\n",
    "    def process_one(self,item):\n",
    "        if isinstance(item, EmptyLabel): return item\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        vsize = len(self.vocab.itos)\n",
    "        res = np.zeros(vsize, dtype=np.int64)\n",
    "        for x in item:\n",
    "            res[self.vocab.stoi[x]] = 1\n",
    "        if res is None: self.warns.append(str(item))\n",
    "        return res\n",
    "    \n",
    "    def process(self, ds):\n",
    "        if self.vocab is None: self.vocab = Vocab.create(ds.items, 2000, 3)\n",
    "        ds.vocab = self.vocab\n",
    "        super().process(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlist = TextList.from_df(df, cols=['sent'], processor=[TokenizeProcessor(include_bos=False),BoWProcessor()])\n",
    "split_data = tlist.split_by_rand_pct()\n",
    "labeled_data = split_data.label_from_df(cols='isvalid')\n",
    "textdb = labeled_data.databunch()\n",
    "len(textdb.train_ds.x.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(data=textdb, model=CommonSense_NN(vocab_size=len(textdb.train_ds.x.vocab.itos)), loss_func=nn.MSELoss())\n",
    "learner.lr_find()\n",
    "learner.recorder.plot()\n",
    "learner.fit_one_cycle(5,1e-06)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Using BERT to get document embeddings #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "unk = 0\n",
    "for sent in df.sent:\n",
    "    sent = sent.lower()\n",
    "    tokens = word_tokenize(sent)\n",
    "    for token in tokens :\n",
    "        if not tokenizer.vocab.get(token):\n",
    "            unk += 1\n",
    "print(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/goelprat/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "bertdf = df.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")\n",
    "testbertdf = testdf.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_tensors = []\n",
    "for sent in bertdf:\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(sent)\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    train_tensors.append(tokens_tensor)\n",
    "    \n",
    "test_tensors = []\n",
    "for sent in testbertdf:\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(sent)\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    test_tensors.append(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1037, 21025, 27528,  7959,  2003,  1037,  2711,  1012,   102]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'a', 'gi', '##raf', '##fe', 'is', 'a', 'person', '.', '[SEP]']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([  101,  1037, 21025, 27528,  7959,  2003,  1037,  2711,  1012,   102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/goelprat/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/goelprat/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "X_test = np.zeros((4042, 768))\n",
    "with torch.no_grad():\n",
    "    for i in range(4042):\n",
    "        tensor = test_tensors[i]\n",
    "        outputs = model(tensor)\n",
    "        embeddings = outputs[0].squeeze()\n",
    "        doc = torch.mean(embeddings.squeeze(), axis=0)\n",
    "        X_test[i] = doc\n",
    "        \n",
    "X = np.zeros((20000, 768))\n",
    "with torch.no_grad():\n",
    "    for i in range(20000):\n",
    "        tensor = train_tensors[i]\n",
    "        outputs = model(tensor)\n",
    "        embeddings = outputs[0].squeeze()\n",
    "        doc = torch.mean(embeddings.squeeze(), axis=0)\n",
    "        X[i] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.isvalid\n",
    "Y_test = testdf.isvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goelprat/miniconda3/envs/fastai/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6358238495794162"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = classifier.score(X_test, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Using BERT to get perplexity #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [CLS] He poured orange juice on his cereal. [SEP]\n",
       "1                [CLS] He poured milk on his cereal. [SEP]\n",
       "2                             [CLS] He drinks apple. [SEP]\n",
       "3                              [CLS] He drinks milk. [SEP]\n",
       "4                        [CLS] Jeff ran a mile today [SEP]\n",
       "                               ...                        \n",
       "19995    [CLS] Harry went to the barbershop to have his...\n",
       "19996          [CLS] Reilly is sleeping on the couch [SEP]\n",
       "19997         [CLS] Reilly is sleeping on the window [SEP]\n",
       "19998                 [CLS] I have a desk on my lamp [SEP]\n",
       "19999                 [CLS] I have a lamp on my desk [SEP]\n",
       "Name: sent, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertdf = df.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")\n",
    "bertdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/goelprat/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_tensors = []\n",
    "for sent in bertdf:\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(sent)\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    train_tensors.append(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] He poured orange juice on his cereal. [SEP]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/goelprat/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/goelprat/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.5285578190358\n",
      "40.22448497557022\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def get_score():\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    for tensor in train_tensors[:2] :\n",
    "        outputs = model(tensor)\n",
    "        predictions = outputs[0]\n",
    "        # confirm we were able to predict 'henson'\n",
    "        tokens = []\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(predictions.squeeze(),tensor.squeeze()).data \n",
    "        perp = math.exp(loss)\n",
    "        print(perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Finetuning BERT using transformers library #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [CLS] He poured orange juice on his cereal. [SEP]\n",
       "1                [CLS] He poured milk on his cereal. [SEP]\n",
       "2                             [CLS] He drinks apple. [SEP]\n",
       "3                              [CLS] He drinks milk. [SEP]\n",
       "4                        [CLS] Jeff ran a mile today [SEP]\n",
       "                               ...                        \n",
       "19995    [CLS] Harry went to the barbershop to have his...\n",
       "19996          [CLS] Reilly is sleeping on the couch [SEP]\n",
       "19997         [CLS] Reilly is sleeping on the window [SEP]\n",
       "19998                 [CLS] I have a desk on my lamp [SEP]\n",
       "19999                 [CLS] I have a lamp on my desk [SEP]\n",
       "Name: sent, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertdf = df.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")\n",
    "bertdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_tensors = []\n",
    "for sent in df.sent:\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(sent)\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    train_tensors.append(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MyBertForSequenceClassification(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_labels=2):\n",
    "        super(MyBertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/goelprat/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "max_seq_length = 50\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list):\n",
    "        self.x_y_list = x_y_list\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
    "        if len(tokenized_review) > max_seq_length:\n",
    "            tokenized_review = tokenized_review[:max_seq_length]\n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "        padding = [0] * (max_seq_length - len(ids_review))       \n",
    "        ids_review += padding      \n",
    "        assert len(ids_review) == max_seq_length       \n",
    "        ids_review = torch.tensor(ids_review)        \n",
    "        sentiment = self.x_y_list[1][index] # color        \n",
    "        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
    "        return ids_review, list_of_labels[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            sentiment_corrects = 0\n",
    "                        \n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders_dict[phase]: \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = F.softmax(outputs,dim=1)  \n",
    "                    loss = criterion(outputs, targets.long())\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                print('Running loss : {:.4f}'.format(running_loss))\n",
    "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == targets.long())\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} sentiment_acc: {:.4f}'.format(\n",
    "                phase, sentiment_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_lists = [df.sent[:1000], df.isvalid[:1000]]\n",
    "test_lists = [testdf.sent[:100], testdf.isvalid[:100]]\n",
    "training_dataset = text_dataset(x_y_list = train_lists)\n",
    "test_dataset = text_dataset(x_y_list = test_lists)\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(test_lists[0])}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyBertForSequenceClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "optim1 = optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "       \n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/4\n",
      "----------\n",
      "Running loss : 12.3019\n",
      "Running loss : 23.1574\n",
      "Running loss : 34.4371\n",
      "Running loss : 47.0812\n",
      "Running loss : 58.0687\n",
      "Running loss : 69.9240\n",
      "Running loss : 79.8553\n",
      "Running loss : 90.6626\n",
      "Running loss : 98.7249\n",
      "Running loss : 108.6554\n",
      "Running loss : 121.1563\n",
      "Running loss : 133.6704\n",
      "Running loss : 147.1135\n",
      "Running loss : 160.5545\n",
      "Running loss : 173.0695\n",
      "Running loss : 187.0734\n",
      "Running loss : 200.6931\n",
      "Running loss : 212.4535\n",
      "Running loss : 225.4815\n",
      "Running loss : 236.5354\n",
      "Running loss : 247.6441\n",
      "Running loss : 259.3732\n",
      "Running loss : 270.5484\n",
      "Running loss : 281.8232\n",
      "Running loss : 292.8590\n",
      "Running loss : 303.2048\n",
      "Running loss : 314.9358\n",
      "Running loss : 324.8700\n",
      "Running loss : 335.8258\n",
      "Running loss : 347.7164\n",
      "Running loss : 359.6016\n",
      "Running loss : 370.9968\n",
      "Running loss : 382.9237\n",
      "Running loss : 393.9334\n",
      "Running loss : 404.9101\n",
      "Running loss : 416.2926\n",
      "Running loss : 427.3474\n",
      "Running loss : 438.9927\n",
      "Running loss : 450.3670\n",
      "Running loss : 461.2991\n",
      "Running loss : 472.6591\n",
      "Running loss : 483.9514\n",
      "Running loss : 495.0947\n",
      "Running loss : 506.8764\n",
      "Running loss : 519.0041\n",
      "Running loss : 529.8201\n",
      "Running loss : 540.6880\n",
      "Running loss : 551.5649\n",
      "Running loss : 562.8761\n",
      "Running loss : 573.8807\n",
      "Running loss : 585.3623\n",
      "Running loss : 596.5070\n",
      "Running loss : 607.7903\n",
      "Running loss : 618.9595\n",
      "Running loss : 630.2666\n",
      "Running loss : 641.0911\n",
      "Running loss : 652.0794\n",
      "Running loss : 663.4553\n",
      "Running loss : 675.8289\n",
      "Running loss : 687.2251\n",
      "Running loss : 698.4349\n",
      "Running loss : 710.0509\n",
      "Running loss : 715.6568\n",
      "train total loss: 0.7157 \n",
      "train sentiment_acc: 0.4910\n",
      "Epoch 1/4\n",
      "----------\n",
      "Running loss : 11.0480\n",
      "Running loss : 22.1979\n",
      "Running loss : 33.4068\n",
      "Running loss : 44.5447\n",
      "Running loss : 55.6201\n",
      "Running loss : 66.7155\n",
      "Running loss : 77.8856\n",
      "Running loss : 88.8906\n"
     ]
    }
   ],
   "source": [
    "model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.text_dataset at 0x1a89a6f510>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop done\n",
      "Loop done\n",
      "Loop done\n",
      "Loop done\n",
      "Loop done\n",
      "Loop done\n",
      "Loop done\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "results = []\n",
    "for inputs, targets in test_loader:\n",
    "    outputs = model_ft1(inputs)\n",
    "    outputs = F.softmax(outputs,dim=1)\n",
    "    for i in range(len(outputs)):\n",
    "        predicted_index = torch.argmax(outputs[i]).item()\n",
    "        results.append(predicted_index)\n",
    "    print(\"Loop done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760.5078798734681\n",
      "298.18919185123605\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def get_score(tokens_tensor):\n",
    "    predictions = None\n",
    "    # Predict all tokens\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "        loss = loss_fct(predictions.squeeze(),tokens_tensor.squeeze()).data \n",
    "        return math.exp(loss)\n",
    "\n",
    "tokens_tensor = train_tensors[12]\n",
    "print(get_score(tokens_tensor))\n",
    "tokens_tensor = train_tensors[13]\n",
    "print(get_score(tokens_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      he put an elephant into the fridge\n",
       "1                         he put a turkey into the fridge\n",
       "2       my sister eats an apple after breakfast every day\n",
       "3        my sister eats a stone after breakfast every day\n",
       "4                       money can be used for buying cars\n",
       "                              ...                        \n",
       "4037    people have to hold onto their shoes because o...\n",
       "4038    Rebecca wakes up because she takes sleeping pills\n",
       "4039           Rebecca wakes up because she sets an alarm\n",
       "4040                      people bleed when they feel hot\n",
       "4041                      people sweat when they feel hot\n",
       "Name: sent, Length: 4042, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2002,  8542,  6501,  2006,  2010, 20943,  1012,   102])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/goelprat/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transdf = testdf.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "test_tensors = []\n",
    "for sent in transdf:\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(sent)\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    test_tensors.append(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] my sister eats a stone after breakfast every day [SEP]'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transdf[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " 'sister',\n",
       " 'eats',\n",
       " 'a',\n",
       " 'stone',\n",
       " 'after',\n",
       " 'breakfast',\n",
       " 'every',\n",
       " 'day',\n",
       " '.']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(test_tensors[3])\n",
    "predictions = outputs[0]\n",
    "predictions = predictions.squeeze()\n",
    "predicted_tokens = []\n",
    "for i in range(len(predictions)):\n",
    "    predicted_index = torch.argmax(predictions[i]).item()\n",
    "    predicted_tokens.append(tokenizer.convert_ids_to_tokens([predicted_index])[0])\n",
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-616d607e260f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-dafb52d2e147>\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(tokens_tensor)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, masked_lm_labels)\u001b[0m\n\u001b[1;32m    768\u001b[0m                             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                             head_mask=head_mask)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    625\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[1;32m    626\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                        head_mask=head_mask)\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(0, len(test_tensors), 2):\n",
    "    t1 = test_tensors[i]\n",
    "    t2 = test_tensors[i + 1]\n",
    "    s1 = get_score(t1)\n",
    "    s2 = get_score(t2)\n",
    "    if s1 <= s2:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          2\n",
       "2          3\n",
       "3          4\n",
       "4          5\n",
       "        ... \n",
       "2016    2017\n",
       "2017    2018\n",
       "2018    2019\n",
       "2019    2020\n",
       "2020    2021\n",
       "Name: id, Length: 2021, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([x_test.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval/taskA_scorer.py --gold-labels data/train/subtaskA_answers_all.csv --pred-labels predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.7279%\r\n"
     ]
    }
   ],
   "source": [
    "!python eval/taskA_scorer.py --gold-labels data/trial/taskA_trial_answer.csv --pred-labels predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
