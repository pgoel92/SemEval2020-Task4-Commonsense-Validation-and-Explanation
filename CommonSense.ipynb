{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSuNtjz48w4K"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "x_train = pd.read_csv('data/train/subtaskA_data_all.csv')\n",
    "y_train = pd.read_csv('data/train/subtaskA_answers_all.csv', header=None)\n",
    "x_dev = pd.read_csv('data/dev/subtaskA_dev_data.csv')\n",
    "y_dev = pd.read_csv('data/dev/subtaskA_gold_answers.csv', header=None)\n",
    "x_test = pd.read_csv('data/trial/taskA_trial_data.csv')\n",
    "y_test = pd.read_csv('data/trial/taskA_trial_answer.csv', header=None)\n",
    "x_submit = pd.read_csv('data/test/subtaskA_test_data.csv')\n",
    "y_train = y_train.rename(columns={0 : 'id',1: 'invalid_sent'})\n",
    "y_dev = y_dev.rename(columns={0 : 'id',1: 'invalid_sent'})\n",
    "y_test = y_test.rename(columns={0 : 'id',1: 'invalid_sent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9p5V2Xd8w4S"
   },
   "outputs": [],
   "source": [
    "xy = x_train.merge(y_train)\n",
    "items = []\n",
    "for row in xy.itertuples():\n",
    "    if row.invalid_sent == 0:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 0.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 1.0})\n",
    "    else:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 1.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 0.0})\n",
    "traindf = pd.DataFrame(items)\n",
    "\n",
    "xytest = x_test.merge(y_test)\n",
    "items = []\n",
    "for row in xytest.itertuples():\n",
    "    if row.invalid_sent == 0:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 0.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 1.0})\n",
    "    else:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 1.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 0.0})\n",
    "        \n",
    "testdf = pd.DataFrame(items)\n",
    "\n",
    "xydev = x_dev.merge(y_dev)\n",
    "items = []\n",
    "for row in xydev.itertuples():\n",
    "    if row.invalid_sent == 0:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 0.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 1.0})\n",
    "    else:\n",
    "        items.append({'id' : row.id, 'sent' : row.sent0, 'isvalid' : 1.0})\n",
    "        items.append({'id' : row.id, 'sent' : row.sent1, 'isvalid' : 0.0})\n",
    "devdf = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8xbxuTo8w4W"
   },
   "source": [
    "## Baseline - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UFlCOu1O8w4X",
    "outputId": "4e416d86-6805-425b-df4e-ad145261d93e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/goelprat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/goelprat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def preprocess(x):\n",
    "    x = x.lower()\n",
    "    tokens = word_tokenize(x)\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words] \n",
    "    return ' '.join([ps.stem(t) for t in filtered_tokens])\n",
    "trainpdf = traindf.sent.apply(preprocess)\n",
    "testpdf = testdf.sent.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXh3z2sE8w4f",
    "outputId": "0a642d7c-5e40-409a-d474-7aacb50b2f08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False, max_features=3000)\n",
    "vectorizer.fit(trainpdf)\n",
    "len(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GAxRpUW8w4i"
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(trainpdf)\n",
    "X_test = vectorizer.transform(testpdf)\n",
    "Y_train = traindf.isvalid\n",
    "Y_test = testdf.isvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOGDfHDF8w4l",
    "outputId": "bfd2120d-1357-4422-9c20-9d026085a1e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kj5Z39Ed8w4o",
    "outputId": "ba6c60fa-8131-4ec7-cefb-710aafdee4c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774369124195943"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = classifier.score(X_test, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AX3KoZl98w4r"
   },
   "source": [
    "Baseline accuracy is 57%. I now use Keras to essentially do simple logistic regression just to practice grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgYIYQUb8w4s",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Answer 5\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "### Answer 3\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model(optimizer='sgd'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid', input_dim = 3000))\n",
    "    # Describe the loss and how it is optimized\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def grid_search():\n",
    "    optimizers = []\n",
    "    model = KerasClassifier(build_fn=create_model)\n",
    "    optimizer = ['SGD', 'Adam', 'RMSProp']\n",
    "    param_grid = dict(epochs=[20, 40, 60], batch_size=[20, 40, 60], optimizer=optimizer)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        \n",
    "model = create_model(optimizer='adam')\n",
    "model.fit(X_train, Y_train, epochs=40, batch_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3vml1gQ8w4w",
    "outputId": "acfe45c4-bebc-4b3e-9dde-0e6f8fa8516f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4042/4042 [==============================] - 0s 57us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24135216026539735, 0.5821375846862793]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5jaUWFq8w40"
   },
   "source": [
    "## Using BERT with pretrained weights to get sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbipxZcX8w42"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8Cey0dP8w45"
   },
   "outputs": [],
   "source": [
    "def bert_input(df):\n",
    "    bertdf = df.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")\n",
    "    bertdf_tokenized = bertdf.apply((lambda x: tokenizer.encode(x)))\n",
    "    max_len = 0\n",
    "    for i in bertdf_tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in bertdf_tokenized.values])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gjo-Lg028w4_"
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "input_ids_train, attention_mask_train = bert_input(traindf)\n",
    "with torch.no_grad():\n",
    "    hidden_train = model(input_ids_train, attention_mask=attention_mask_train)\n",
    "features_train = hidden_train[0][:,0,:].numpy()\n",
    "\n",
    "input_ids_test, attention_mask_test = bert_input(testdf)\n",
    "with torch.no_grad():\n",
    "    hidden_test = model(input_ids_test, attention_mask=attention_mask_test)\n",
    "features_test = hidden_test[0][:,0,:].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCSo-4zz8w5E"
   },
   "outputs": [],
   "source": [
    "Y_train = traindf.isvalid\n",
    "Y_test = testdf.isvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4v8iia-e8w5G",
    "outputId": "fed59d35-8fc1-4e61-ae10-4cd9bccf7eab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goelprat/miniconda3/envs/fastai/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(features_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vmXB4JbK8w5M",
    "outputId": "1e93080d-5460-4d5d-b015-d913ae0fa492"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6264225630875804"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = classifier.score(features_test, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "picm96sP8w5Q"
   },
   "source": [
    "## Using BERT to get perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9nAah3fW8w5R"
   },
   "source": [
    "In the [paper](https://arxiv.org/abs/1906.00363) published along with this task, the authors claim to get 70% accuracy by using BERT to get sentence perplexity. I try to replicate that claim in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYDjI9DY8w5R"
   },
   "outputs": [],
   "source": [
    "bertdf = traindf.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")\n",
    "testbertdf = testdf.sent.apply(lambda x : \"[CLS] \" + x + \" [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def get_bert_inputs(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    for i, row in df.iterrows():\n",
    "        tokendict = tokenizer.encode_plus(row['sent'], max_length=25, pad_to_max_length=True)\n",
    "        input_ids.append(tokendict['input_ids'])\n",
    "        attention_masks.append(tokendict['attention_mask'])\n",
    "        token_type_ids.append(tokendict['token_type_ids'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks, token_type_ids = get_bert_inputs(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmBnfRQS8w5U",
    "outputId": "82bc22a9-b11f-4a73-a130-6e3692c87b68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/goelprat/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_tensors = []\n",
    "for sent in bertdf:\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(sent)\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    train_tensors.append(tokens_tensor)\n",
    "    \n",
    "test_tensors = []\n",
    "for sent in testbertdf:\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(sent)\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    test_tensors.append(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyZEwimz8w5W",
    "outputId": "3ad62b3a-d9e6-4a72-cbb4-17f557a135c7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHJvIRh08w5Z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def run_tokens(s):\n",
    "    tokenized_text = tokenizer.tokenize(s)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_ids = [0] * tokens_tensor.shape[1]\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    prediction = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = prediction[0]\n",
    "    predicted_tokens = []\n",
    "    for i in range(tokens_tensor.shape[1]):\n",
    "        predicted_index = torch.argmax(predictions[0, i]).item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    return predicted_tokens\n",
    "\n",
    "def run_perp(s):\n",
    "    tokenized_text = tokenizer.tokenize(s)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_ids = [0] * tokens_tensor.shape[1]\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    predictions = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    loss = loss_fct(predictions[0].squeeze(),tokens_tensor.squeeze()).data \n",
    "    return math.exp(loss)\n",
    "\n",
    "def run(sents, mode):\n",
    "    model.eval()\n",
    "    f = run_perp if mode == 'perp' else run_tokens\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for sent in sents:\n",
    "            out.append(f(sent))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list):\n",
    "        self.x_y_list = x_y_list\n",
    "        \n",
    "    def __getitem__(self,index):           \n",
    "        input_ids = self.x_y_list[0][index]\n",
    "        token_type_ids = self.x_y_list[1][index]\n",
    "        attention_mask = self.x_y_list[2][index]\n",
    "        label = self.x_y_list[3][index]\n",
    "        return input_ids, token_type_ids, attention_mask, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "isvalid = torch.tensor(testdf.isvalid)\n",
    "test_lists = [input_ids, attention_masks, token_type_ids, isvalid]\n",
    "test_dataset = text_dataset(x_y_list = test_lists)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "perps = []\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "for inputs, masks, token_types, target in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    #masks = masks.to(device)\n",
    "    #token_types = token_types.to(device)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    output = model(inputs, masked_lm_labels=inputs)\n",
    "    #loss = loss_fct(output[0].squeeze(),inputs.squeeze()).data \n",
    "    perps.append(math.exp(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he was sent to a restaurant for treatment after a car crash 53.811206193880054\n",
      "he was sent to a hospital for treatment after a car crash 56.52741072294196\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "sent0 = testdf.sent[i]\n",
    "sent1 = testdf.sent[i + 1]\n",
    "input_ids0 = torch.tensor(tokenizer.encode(sent0, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "input_ids1 = torch.tensor(tokenizer.encode(sent1, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "outputs0 = model(input_ids0, masked_lm_labels=input_ids0)\n",
    "outputs1 = model(input_ids1, masked_lm_labels=input_ids1)\n",
    "\n",
    "print(sent0, math.exp(outputs0[0]))\n",
    "print(sent1, math.exp(outputs1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'money can be used for buying cars'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.sent[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alOF0Lz78w5d",
    "outputId": "747d9449-09a3-4569-c98b-29be741cad86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42.116081023755655, 38.70850197316724]"
      ]
     },
     "execution_count": 133,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = ['[CLS] my sister eats an apple after breakfast every day . [SEP]', '[CLS] my sister eats a stone after breakfast every day . [SEP]']\n",
    "run(sents, 'perp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-52BX8IJ8w5g"
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for s in testdf.sent:\n",
    "    sents.append('[CLS] ' + s + ' . [SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7q8TibEd8w5j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perps = run(sents, 'perp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49SW3n-d8w5l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_results_from_perps(perps):\n",
    "    results = []\n",
    "    for i in range(0, len(perps), 2):\n",
    "        if perps[i] < perps[i + 1]:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    return results\n",
    "\n",
    "results = get_results_from_perps(perps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.4805%\r\n"
     ]
    }
   ],
   "source": [
    "pd.concat([x_test.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)\n",
    "!python3 eval/taskA_scorer.py --gold-labels data/trial/taskA_trial_answer.csv --pred-labels predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfpPSBrU8w5o"
   },
   "source": [
    "I was unable to get better than random results using BERT to get sentence probability. I believe this is because BERT outputs probability conditional on all the other tokens. This means that multiplying the probabilities for tokens in a sentence does not yield the probability of the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOVYL3gG8w5p"
   },
   "source": [
    "## Using GPT-2 to get sentence probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xeTX5Kyq8w5q",
    "outputId": "1aca1e55-5625-4df6-c27c-cc26e8784287",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model\n",
    "gpt_model = GPT2Model.from_pretrained('gpt2')\n",
    "gpt_model.eval()\n",
    "# Load pre-trained model (weights)\n",
    "gpt_model_lm = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "gpt_model_lm.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "loss_fct = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeXgm52Y8w5t"
   },
   "outputs": [],
   "source": [
    "def gpt_input(df):\n",
    "    gptdf = df.sent.apply(lambda x : ' <|endoftext|> ' + x)\n",
    "    gptdf_tokenized = gptdf.apply((lambda x: tokenizer.encode(x)))\n",
    "    max_len = 0\n",
    "    for i in gptdf_tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in gptdf_tokenized.values])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2j5SYXW8w5x"
   },
   "outputs": [],
   "source": [
    "def gpt_predictions(sentence, model):\n",
    "    sentence = ' <|endoftext|> ' + sentence\n",
    "    tokenize_input = tokenizer.tokenize(sentence, add_prefix_space=True)\n",
    "    tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    return model(tokens_tensor)\n",
    "    \n",
    "def get_last_hidden_state(sentence, model):\n",
    "    predictions = gpt_predictions(sentence, model)\n",
    "    return predictions[0].squeeze()[-1]\n",
    "\n",
    "def gpt_score(sentence):\n",
    "    sentence = ' <|endoftext|> ' + sentence\n",
    "    tokenize_input = tokenizer.tokenize(sentence, add_prefix_space=True)\n",
    "    tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    predictions=gpt_model_lm(tokens_tensor)\n",
    "    loss = loss_fct(predictions[0].squeeze()[:-1],tokens_tensor.squeeze()[1:]).data \n",
    "    return math.exp(loss)\n",
    "\n",
    "def gpt_tokens(input_sentence):\n",
    "    sentence = '<|endoftext|> ' + input_sentence\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    predictions =gpt_model(tokens_tensor)\n",
    "    predicted_tokens = []\n",
    "    for i in range(tokens_tensor.shape[1]):\n",
    "        predicted_index = torch.argmax(predictions[0][0, i]).item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    return predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPij7uPM8w50"
   },
   "outputs": [],
   "source": [
    "def run(sents, mode='perp'):\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for sent in sents:\n",
    "            if mode == 'perp':\n",
    "                out.append(gpt_score(sent))\n",
    "            else:\n",
    "                out.append(gpt_tokens(sent))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5BwLnkY8w54",
    "outputId": "b4c3e957-7eee-4967-f15b-5d33f31e5337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[628.2036629979144, 448.928654083428]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(['he put an elephant into the fridge', 'he put a turkey into the fridge'], mode='perp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbP0WK508w58"
   },
   "outputs": [],
   "source": [
    "input_ids, attention_mask = gpt_input(testdf)\n",
    "with torch.no_grad():\n",
    "    predictions = gpt_model_lm(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FM9AzBU78w5_",
    "outputId": "f3643fc9-dbe9-4196-b3cb-41b1e42cd371"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.6329864510042"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "loss = loss_fct(predictions[0][i, :-1, :],input_ids[i, 1:]).data \n",
    "math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idaiWkbm8w6E"
   },
   "outputs": [],
   "source": [
    "perps = []\n",
    "for i in range(4042):\n",
    "    loss = loss_fct(predictions[0][i, :-1, :],input_ids[i, 1:]).data \n",
    "    perps.append(math.exp(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_xKBiKR8w6G"
   },
   "outputs": [],
   "source": [
    "def get_results_from_perps(perps):\n",
    "    results = []\n",
    "    for i in range(0, len(perps), 2):\n",
    "        if perps[i] < perps[i + 1]:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    return results\n",
    "\n",
    "results = get_results_from_perps(perps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2gPSP5S8w6T",
    "outputId": "e7066fd3-ba16-4645-a56e-d47696a665b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.3508%\n"
     ]
    }
   ],
   "source": [
    "pd.concat([x_test.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)\n",
    "!python3 eval/taskA_scorer.py --gold-labels data/trial/taskA_trial_answer.csv --pred-labels predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCNOkF6r8w6V"
   },
   "source": [
    "These results using the GPT-2 small model match the ~70% baseline reported in the paper. \n",
    "Using the large model, I was able to get 71.35% accuracy. \n",
    "\n",
    "Next, I experiment with using the final hidden state output of the GPT-2 model to classify a sentence as for or against common sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkYNUGwF8w6V"
   },
   "outputs": [],
   "source": [
    "def run_classification(sents, model):\n",
    "    import numpy as np\n",
    "    x = np.zeros((len(sents), 768))\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sents)):\n",
    "            h = get_last_hidden_state(sents[i], model)\n",
    "            x[i] = h\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8FHAETc58w6X"
   },
   "outputs": [],
   "source": [
    "sents = [x for x in traindf.sent]\n",
    "x = run_classification(sents, gpt_model)\n",
    "y = traindf.isvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sECj0Sya8w6e",
    "outputId": "2e63ad1f-8e82-48b6-df3c-8be757384d04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goelprat/miniconda3/envs/fastai/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--2fadnJ8w6l"
   },
   "outputs": [],
   "source": [
    "x_test = run_classification([x for x in testdf.sent], gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdLbrggv8w6n"
   },
   "outputs": [],
   "source": [
    "y_test = testdf.isvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KYtmIuPC8w6q",
    "outputId": "180d6eaf-10f0-46e2-a2d7-64fcaf8d2b4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.591044037605146"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGsTmQ1m8w6v"
   },
   "source": [
    "Now, I try to find examples that are incorrectly classified by my GPT-2 based model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMlL0lQV8w6y"
   },
   "outputs": [],
   "source": [
    "test_answers = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/trial/taskA_trial_answer.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLrwcf928w60",
    "outputId": "c17180a4-1212-4276-8502-182707dfd5ba"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ytrk2h2_8w67"
   },
   "outputs": [],
   "source": [
    "wrong_examples = x_test[test_answers[1] != pd.Series(results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_O12LPL8w69"
   },
   "outputs": [],
   "source": [
    "items = []\n",
    "for row in wrong_examples.itertuples():\n",
    "    items.append({'id' : row.id, 'sent' : row.sent0})\n",
    "    items.append({'id' : row.id, 'sent' : row.sent1})\n",
    "wrongdf = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "wBQYDP0K8w6_",
    "outputId": "b695002a-335d-4874-818a-21a077640250"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a man can better see stars and the moon in day...\n",
       "1       a man can hardly see stars and the moon in day...\n",
       "2                                   I work 25 hours a day\n",
       "3                                    I work 8 hours a day\n",
       "4        I changed my direction when passing a crossroads\n",
       "                              ...                        \n",
       "1273                    Jim downloads music from the book\n",
       "1274              Bob goes to bed because he feels sleepy\n",
       "1275             Bob goes to work because he feels sleepy\n",
       "1276    people have to hold onto their hats because of...\n",
       "1277    people have to hold onto their shoes because o...\n",
       "Name: sent, Length: 1278, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongdf.sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "a3WtdHQm8w7B",
    "outputId": "7c611ebe-116b-4666-ea3e-8a5f60e6f296"
   },
   "outputs": [],
   "source": [
    "with open('wrong_examples.txt', 'w') as f:\n",
    "    for s in wrongdf.sent:\n",
    "        f.write(str(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWQvvFSF8w7F"
   },
   "source": [
    "## Finetuning BERT using transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "2cde83d55edf427fa2fbf3e535beac09",
      "197df6e7f7724935994042f0dec842a4",
      "4903ddedfef04516b44f534517d07851",
      "46f014b39fad4d0f9dfdaec07c10df3e",
      "ef49a5d738ba4cbe8c5996a099f460ed",
      "584bead4df0943cbbd1663243d2bd679",
      "f1ac8a5b3f7e420b97d76fbd536b435b",
      "03d93305b3754704ac24e5d960c6c5e2"
     ]
    },
    "colab_type": "code",
    "id": "FBCgSLIm8w7J",
    "outputId": "9333bf5a-32b2-4432-d2a7-f4ff611f541e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def get_bert_inputs(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    for i, row in df.iterrows():\n",
    "        tokendict = tokenizer.encode_plus(row['sent0'], row['sent1'], max_length=50, pad_to_max_length=True)\n",
    "        input_ids.append(tokendict['input_ids'])\n",
    "        attention_masks.append(tokendict['attention_mask'])\n",
    "        token_type_ids.append(tokendict['token_type_ids'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avTpiHBQ8w7L"
   },
   "outputs": [],
   "source": [
    "input_ids, attention_masks, token_type_ids = get_bert_inputs(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_dev, attention_masks_dev, token_type_ids_dev = get_bert_inputs(xydev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Un5JbKlK8w7N"
   },
   "outputs": [],
   "source": [
    "input_ids_test, attention_masks_test, token_type_ids_test = get_bert_inputs(xytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Pj49Yyc8w7O"
   },
   "outputs": [],
   "source": [
    "class MyBertForSequenceClassification(torch.nn.Module):  \n",
    "    def __init__(self, num_labels=1):\n",
    "        super(MyBertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-large-uncased')\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        return pooled_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrv_xa5K8w7Q"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "max_seq_length = 50\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list):\n",
    "        self.x_y_list = x_y_list\n",
    "        \n",
    "    def __getitem__(self,index):           \n",
    "        input_ids = self.x_y_list[0][index]\n",
    "        token_type_ids = self.x_y_list[1][index]\n",
    "        attention_mask = self.x_y_list[2][index]\n",
    "        label = self.x_y_list[3][index]\n",
    "        return input_ids, token_type_ids, attention_mask, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjHGz8x68w7T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, device, num_epochs=25):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                print(\"VALIDATION\")\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            corrects = 0\n",
    "                        \n",
    "            # Iterate over data.\n",
    "            for inputs, token_types, mask, target in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                token_types = token_types.to(device)\n",
    "                mask = mask.to(device)\n",
    "                target = target.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, token_type_ids=token_types, attention_mask=mask)\n",
    "                    outputs = F.softmax(outputs,dim=1)  \n",
    "                    loss = criterion(outputs, target)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #print('Running loss : {:.4f}'.format(running_loss))\n",
    "                corrects += torch.sum(torch.max(outputs, 1)[1] == target)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            acc = corrects.double() / dataset_sizes[phase]\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} accuracy: {:.4f}'.format(\n",
    "                phase, acc))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9Y3-7Dl8w7V"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 16\n",
    "invalid = torch.tensor(xy.invalid_sent)\n",
    "invalid_dev = torch.tensor(xydev.invalid_sent)\n",
    "#train_indices, val_indices = train_test_split([i for i in range(10000)])\n",
    "#train_lists = [input_ids[train_indices], token_type_ids[train_indices], attention_masks[train_indices], invalid[train_indices]]\n",
    "#val_lists = [input_ids[val_indices], token_type_ids[val_indices], attention_masks[val_indices], invalid[val_indices]]\n",
    "train_lists = [input_ids, token_type_ids, attention_masks, invalid]\n",
    "val_lists = [input_ids_dev, token_type_ids_dev, attention_masks_dev, invalid_dev]\n",
    "test_lists = [input_ids_test, token_type_ids_test, attention_masks_test, invalid]\n",
    "training_dataset = text_dataset(x_y_list = train_lists)\n",
    "val_dataset = text_dataset(x_y_list = val_lists)\n",
    "test_dataset = text_dataset(x_y_list = test_lists)\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(val_lists[0])}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "asghP-Ji_ES-",
    "outputId": "3a853891-3f1f-41dd-a102-885e2ddaa919"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "0c13faf3c61d4e739b43d3b2e2b0e4a2",
      "1e07d8379d064f7c8fef772afe0a7ac2",
      "a7f9ba7b72ca4e9fbfd1995f9f046cf7",
      "a564e74ce99b414bb2b3a288370abae0",
      "b263c5db98604c4195acaddc8815fa88",
      "67b267518b1a46b4ad81f50797c1c6e1",
      "049d01a118444e5aaf684d467c0d07a6",
      "4c5f563401e840da9afd13fea555d516",
      "00492598dbd542c8a84592a5ac065991",
      "35cdabdbef024a6b81331ea10c614365",
      "5b56c4372bcb459ea8418eeabc92d7fc",
      "58ecfe855eef4271912a91187990e6fb",
      "26c778e84160499eaff2f073b05eec3b",
      "931405db76534b1db2a3a82103385cbf",
      "f85cef55a5724c5ab574f1c09f770d82",
      "663b3f4172b94ca98738f24923b0fc02"
     ]
    },
    "colab_type": "code",
    "id": "cPEvNpXg8w7X",
    "outputId": "86c908e9-d49c-4afd-8fb3-28dff2d46aa8"
   },
   "outputs": [],
   "source": [
    "model = MyBertForSequenceClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a049Q2CV8w7b"
   },
   "outputs": [],
   "source": [
    "#from torch import optim\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 3\n",
    "lrlast = .001\n",
    "lrmain = 2e-5\n",
    "optim = AdamW(model.bert.parameters(), lr=lrmain, eps=1e-8)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = (len(train_lists[0]) / batch_size) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YAuAqzJB8w7e",
    "outputId": "afbd6ecb-2a60-484d-9fce-b15c8a8688cd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/2\n",
      "----------\n",
      "train total loss: 0.6208 \n",
      "train accuracy: 0.6521\n",
      "VALIDATION\n",
      "val total loss: 0.5578 \n",
      "val accuracy: 0.7422\n",
      "saving with loss of 0.5577636059810788 improved over previous 100\n",
      "Epoch 1/2\n",
      "----------\n",
      "train total loss: 0.4933 \n",
      "train accuracy: 0.8118\n",
      "VALIDATION\n",
      "val total loss: 0.5051 \n",
      "val accuracy: 0.7974\n",
      "saving with loss of 0.5051305820136037 improved over previous 0.5577636059810788\n",
      "Epoch 2/2\n",
      "----------\n",
      "train total loss: 0.4240 \n",
      "train accuracy: 0.8858\n",
      "VALIDATION\n",
      "val total loss: 0.5033 \n",
      "val accuracy: 0.8074\n",
      "saving with loss of 0.503279189587596 improved over previous 0.5051305820136037\n",
      "Training complete in 36m 6s\n"
     ]
    }
   ],
   "source": [
    "model_ft1 = train_model(model, criterion, optim, scheduler, device, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVuTH6_08w7g"
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "results = []\n",
    "for ipt, toktypes, maskt, targett in test_loader:\n",
    "    ipt = ipt.to(device)\n",
    "    toktypes = toktypes.to(device)\n",
    "    maskt = maskt.to(device)\n",
    "    outputs = model_ft1(ipt, toktypes, maskt)\n",
    "    outputs = F.softmax(outputs,dim=1)\n",
    "    for i in range(len(outputs)):\n",
    "        predicted_index = torch.argmax(outputs[i]).item()\n",
    "        results.append(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ_1LpCS8w7i",
    "outputId": "e7d116d7-24ae-4916-ee1b-74fc2103341d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rtfNuoys8w7k",
    "outputId": "387c8db6-6b92-470f-d200-3c4ffa31753c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.6764%\r\n"
     ]
    }
   ],
   "source": [
    "pd.concat([x_test.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)\n",
    "!python3 eval/taskA_scorer.py --gold-labels data/trial/taskA_trial_answer.csv --pred-labels predictions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OilIrc-GZ6L9"
   },
   "outputs": [],
   "source": [
    "test_answers = pd.read_csv('data/trial/taskA_trial_answer.csv', header=None)\n",
    "wrong_examples = x_test[test_answers[1] != pd.Series(results)]\n",
    "items = []\n",
    "for row in wrong_examples.itertuples():\n",
    "    items.append({'id' : row.id, 'sent' : row.sent0})\n",
    "    items.append({'id' : row.id, 'sent' : row.sent1})\n",
    "wrongdf = pd.DataFrame(items)\n",
    "with open('wrong_examples.txt', 'w') as f:\n",
    "    for s in wrongdf.sent:\n",
    "        f.write(str(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNPogkrF2I1u"
   },
   "source": [
    "# Finetuning RoBERTa using [CLS] embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eCNccjw30UU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "def get_bert_inputs(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i, row in df.iterrows():\n",
    "        tokendict = tokenizer.encode_plus(row['sent0'], row['sent1'], max_length=50, pad_to_max_length=True)\n",
    "        input_ids.append(tokendict['input_ids'])\n",
    "        attention_masks.append(tokendict['attention_mask'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwnEq2HQ3-Qj"
   },
   "outputs": [],
   "source": [
    "input_ids, attention_masks = get_bert_inputs(xy)\n",
    "input_ids_dev, attention_masks_dev = get_bert_inputs(xydev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcLBnYOz2HL8"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "class MyRoBertaForSequenceClassification(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_labels=1):\n",
    "        super(MyRoBertaForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "        #self.classifier = nn.Linear(768, num_labels)\n",
    "        #nn.init.xavier_normal_(self.classifier.weight)\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        pooled_output = self.bert(input_ids, attention_mask)\n",
    "        return pooled_output[0]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyRoBertaForSequenceClassification().to(device)\n",
    "#model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtr2PCkB6Qbn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "max_seq_length = 50\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list):\n",
    "        self.x_y_list = x_y_list\n",
    "        \n",
    "    def __getitem__(self,index):           \n",
    "        input_ids = self.x_y_list[0][index]\n",
    "        attention_mask = self.x_y_list[1][index]\n",
    "        label = self.x_y_list[2][index]\n",
    "        return input_ids, attention_mask, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfD4fTgt3QTK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "invalid = torch.tensor(xy.invalid_sent)\n",
    "invalid_dev = torch.tensor(xydev.invalid_sent)\n",
    "train_lists = [input_ids, attention_masks, invalid]\n",
    "val_lists = [input_ids_dev, attention_masks_dev, invalid_dev]\n",
    "\n",
    "training_dataset = text_dataset(x_y_list = train_lists)\n",
    "val_dataset = text_dataset(x_y_list = val_lists)\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(val_lists[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, device, num_epochs=25, phases=['train', 'val']):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                print(\"VALIDATION\")\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            corrects = 0\n",
    "                        \n",
    "            # Iterate over data.\n",
    "            for inputs, mask, target in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                mask = mask.to(device)\n",
    "                target = target.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, attention_mask=mask)\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    outputs = F.softmax(outputs,dim=1)  \n",
    "                    loss = criterion(outputs, target)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #print('Running loss : {:.4f}'.format(running_loss))\n",
    "                corrects += torch.sum(torch.max(outputs, 1)[1] == target)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            acc = corrects.double() / dataset_sizes[phase]\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} accuracy: {:.4f}'.format(\n",
    "                phase, acc))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eye5z2tC4b72"
   },
   "outputs": [],
   "source": [
    "#from torch import optim\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "lrlast = .001\n",
    "lrmain = 2e-5\n",
    "optim = AdamW(model.bert.parameters(), lr=lrmain, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = (len(train_lists[0]) / batch_size) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "JEOTAQYh4djx",
    "outputId": "b5ac1a53-39fd-4316-f1a6-4fca31f9ab7e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/2\n",
      "----------\n",
      "train total loss: 0.6948 \n",
      "train accuracy: 0.4889\n",
      "VALIDATION\n",
      "val total loss: 0.6947 \n",
      "val accuracy: 0.4804\n",
      "saving with loss of 0.6946904892428828 improved over previous 100\n",
      "Epoch 1/2\n",
      "----------\n",
      "train total loss: 0.6141 \n",
      "train accuracy: 0.6624\n",
      "VALIDATION\n",
      "val total loss: 0.5119 \n",
      "val accuracy: 0.7924\n",
      "saving with loss of 0.5119188002263053 improved over previous 0.6946904892428828\n",
      "Epoch 2/2\n",
      "----------\n",
      "train total loss: 0.4871 \n",
      "train accuracy: 0.8126\n",
      "VALIDATION\n",
      "val total loss: 0.4781 \n",
      "val accuracy: 0.8215\n",
      "saving with loss of 0.4781279314604542 improved over previous 0.5119188002263053\n",
      "Training complete in 11m 34s\n"
     ]
    }
   ],
   "source": [
    "model_ft1 = train_model(model, criterion, optim, scheduler, device, num_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/0\n",
      "----------\n",
      "VALIDATION\n",
      "val total loss: 0.6932 \n",
      "val accuracy: 0.5064\n",
      "saving with loss of 0.6932363882064819 improved over previous 100\n",
      "Training complete in 0m 21s\n"
     ]
    }
   ],
   "source": [
    "model_ft1 = train_model(model, criterion, optim, scheduler, device, num_epochs=1, phases=['val'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9DSmNjr6kg7"
   },
   "outputs": [],
   "source": [
    "invalid_test = torch.tensor(xytest.invalid_sent)\n",
    "input_ids_test, attention_masks_test = get_bert_inputs(xytest)\n",
    "test_lists = [input_ids_test, attention_masks_test, invalid_test]\n",
    "test_dataset = text_dataset(x_y_list = test_lists)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(data_loader):\n",
    "    results = []\n",
    "    for ipt, maskt, targett in data_loader:\n",
    "        ipt = ipt.to(device)\n",
    "        maskt = maskt.to(device)\n",
    "        outputs = model_ft1(ipt, maskt)\n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        for i in range(len(outputs)):\n",
    "            predicted_index = torch.argmax(outputs[i]).item()\n",
    "            results.append(predicted_index)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_results(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FT993iY96tfL",
    "outputId": "9d52638d-d82d-441d-d772-0eaf77420202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.8877%\r\n"
     ]
    }
   ],
   "source": [
    "pd.concat([x_test.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)\n",
    "!python3 eval/taskA_scorer.py --gold-labels data/trial/taskA_trial_answer.csv --pred-labels predictions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval = pd.read_csv('data/test/subtaskA_test_data.csv')\n",
    "input_ids_eval, attention_masks_eval = get_bert_inputs(x_eval)\n",
    "eval_lists = [input_ids_eval, attention_masks_eval, invalid]\n",
    "eval_dataset = text_dataset(x_y_list = eval_lists)\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, num_workers=0)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_results(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([x_eval.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMMmk_m37GOm"
   },
   "outputs": [],
   "source": [
    "test_answers = pd.read_csv('data/trial/taskA_trial_answer.csv', header=None)\n",
    "wrong_examples = x_test[test_answers[1] != pd.Series(results)]\n",
    "items = []\n",
    "for row in wrong_examples.itertuples():\n",
    "    items.append({'id' : row.id, 'sent' : row.sent0})\n",
    "    items.append({'id' : row.id, 'sent' : row.sent1})\n",
    "wrongdf = pd.DataFrame(items)\n",
    "with open('wrong_examples.txt', 'w') as f:\n",
    "    for s in wrongdf.sent:\n",
    "        f.write(str(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM to get sentence embedding from RoBERTa embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import RobertaModel\n",
    "class CommonSenseClassifier(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(CommonSenseClassifier, self).__init__()\n",
    "        #self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        #self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn1 = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "            self.rnn2 = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.lin1 = nn.Linear(ninp, 1)\n",
    "        self.lin2 = nn.Linear(ninp, 1)\n",
    "        self.classifier = nn.Linear(2, 2)\n",
    "        nn.init.xavier_normal_(self.lin1.weight)\n",
    "        nn.init.xavier_normal_(self.lin2.weight)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        #self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifier.bias.data.zero_()\n",
    "        self.classifier.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input1, input2, hidden1, hidden2):\n",
    "        #emb = self.drop(self.encoder(input))\n",
    "        #emb = self.drop(input)\n",
    "        output1, hidden1 = self.rnn1(input1, hidden1)\n",
    "        output2, hidden2 = self.rnn2(input2, hidden2)\n",
    "        #output = self.drop(output)\n",
    "        #decoded = self.decoder(output)\n",
    "        #decoded = decoded.view(-1, self.ntoken)\n",
    "        #return F.log_softmax(decoded, dim=1), hidden\n",
    "        cell1 = hidden1[1].squeeze()\n",
    "        cell2 = hidden2[1].squeeze()\n",
    "        out1 = self.lin1(cell1)\n",
    "        out2 = self.lin2(cell2)\n",
    "        #class_features = torch.cat((cell1, cell2), 1)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        output = torch.cat((out1, out2), 1)\n",
    "        return torch.nn.functional.softmax(output, dim=1), hidden1, hidden2\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert = RobertaModel.from_pretrained('roberta-base')\n",
    "bert = bert.to(device)\n",
    "sense_model = CommonSenseClassifier('LSTM', 768, 768, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_toks = 0\n",
    "for i, row in xytest.iterrows():\n",
    "    ntokens = len(row['sent0'].split())\n",
    "    if ntokens > max_toks:\n",
    "        max_toks = ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce89a2f2a9844a238d97fb33b99664f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30eed4da95b347538d8e6e9e5d73e001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "max_seq_length = 25\n",
    "def get_roberta_inputs(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        tokendict1 = tokenizer.encode_plus(row['sent0'], max_length=max_seq_length, pad_to_max_length=True, add_special_tokens=False)\n",
    "        tokendict2 = tokenizer.encode_plus(row['sent1'], max_length=max_seq_length, pad_to_max_length=True, add_special_tokens=False)\n",
    "        input_ids.append((tokendict1['input_ids'], tokendict2['input_ids']))\n",
    "        attention_masks.append((tokendict1['attention_mask'], tokendict2['attention_mask']))\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list):\n",
    "        self.x_y_list = x_y_list\n",
    "        \n",
    "    def __getitem__(self,index):           \n",
    "        input_ids = self.x_y_list[0][index]\n",
    "        attention_mask = self.x_y_list[1][index]\n",
    "        label = self.x_y_list[2][index]\n",
    "        return input_ids, attention_mask, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xy = xy[:100]\n",
    "#xydev = xydev[:20]\n",
    "input_ids, attention_masks = get_roberta_inputs(xy)\n",
    "input_ids_dev, attention_masks_dev = get_roberta_inputs(xydev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 2, 25])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "invalid = torch.tensor(xy.invalid_sent)\n",
    "invalid_dev = torch.tensor(xydev.invalid_sent)\n",
    "train_indices, val_indices = train_test_split([i for i in range(10000)])\n",
    "#train_lists = [input_ids[train_indices], attention_masks[train_indices], invalid[train_indices]]\n",
    "#val_lists = [input_ids[val_indices], attention_masks[val_indices], invalid[val_indices]]\n",
    "train_lists = [input_ids, attention_masks, invalid]\n",
    "val_lists = [input_ids_dev, attention_masks_dev, invalid_dev]\n",
    "\n",
    "training_dataset = text_dataset(x_y_list = train_lists)\n",
    "val_dataset = text_dataset(x_y_list = val_lists)\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(val_lists[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch import optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "max_lr = 0.001\n",
    "lrmain = 2e-5\n",
    "optim = Adam(sense_model.parameters(), lr=lrmain, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = lr_scheduler.OneCycleLR(optim, max_lr=max_lr, total_steps = int((len(train_lists[0]) / batch_size) * epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2e-05"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrmain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "def train(model, criterion, optimizer, scheduler, device, num_epochs=25, phases=['train', 'val']):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "        model.train()  # Set model to training mode\n",
    "        #running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                print(\"VALIDATION\")\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            corrects = 0\n",
    "            for inputs, mask, target in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device)            # batch_size x max_sent_len\n",
    "                mask = mask.to(device)\n",
    "                target = target.to(device)\n",
    "                hidden1 = model.init_hidden(inputs.size(0))\n",
    "                hidden2 = model.init_hidden(inputs.size(0))\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    bert_outputs = bert(inputs.reshape((inputs.size(0) * 2, max_seq_length)), attention_mask=mask.reshape((inputs.size(0) * 2, max_seq_length)))\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    embeddings = bert_outputs[0].permute(1, 0, 2)\n",
    "                    emb1 = embeddings[:, ::2, :]\n",
    "                    emb2 = embeddings[:, 1::2, :]\n",
    "                    #hidden = model.init_hidden(inputs.size(0))\n",
    "                    #hidden = repackage_hidden(hidden)\n",
    "                    logits, h1, h2 = model(emb1, emb2, hidden1, hidden2)\n",
    "                    loss = criterion(logits, target)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    corrects += torch.sum(torch.max(logits, 1)[1] == target)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            acc = corrects.double() / dataset_sizes[phase]\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} accuracy: {:.4f}'.format(\n",
    "                phase, acc))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0\n",
      "train total loss: 0.4945 \n",
      "train accuracy: 0.8088\n",
      "VALIDATION\n",
      "val total loss: 0.5606 \n",
      "val accuracy: 0.7392\n",
      "saving with loss of 0.5605972791268091 improved over previous 100\n",
      "Epoch 1\n",
      "train total loss: 0.4784 \n",
      "train accuracy: 0.8259\n",
      "VALIDATION\n",
      "val total loss: 0.5562 \n",
      "val accuracy: 0.7462\n",
      "saving with loss of 0.556222177077918 improved over previous 0.5605972791268091\n",
      "Epoch 2\n",
      "train total loss: 0.4620 \n",
      "train accuracy: 0.8460\n",
      "VALIDATION\n",
      "val total loss: 0.5557 \n",
      "val accuracy: 0.7462\n",
      "saving with loss of 0.5557334069442844 improved over previous 0.556222177077918\n",
      "Epoch 3\n",
      "train total loss: 0.4486 \n",
      "train accuracy: 0.8603\n",
      "VALIDATION\n",
      "val total loss: 0.5712 \n",
      "val accuracy: 0.7212\n",
      "Epoch 4\n",
      "train total loss: 0.4379 \n",
      "train accuracy: 0.8699\n",
      "VALIDATION\n",
      "val total loss: 0.5971 \n",
      "val accuracy: 0.7041\n",
      "Epoch 5\n",
      "train total loss: 0.4221 \n",
      "train accuracy: 0.8891\n",
      "VALIDATION\n",
      "val total loss: 0.5680 \n",
      "val accuracy: 0.7212\n",
      "Epoch 6\n",
      "train total loss: 0.4140 \n",
      "train accuracy: 0.8967\n",
      "VALIDATION\n",
      "val total loss: 0.5522 \n",
      "val accuracy: 0.7543\n",
      "saving with loss of 0.5522147106789538 improved over previous 0.5557334069442844\n",
      "Epoch 7\n",
      "train total loss: 0.3954 \n",
      "train accuracy: 0.9159\n",
      "VALIDATION\n",
      "val total loss: 0.5577 \n",
      "val accuracy: 0.7402\n",
      "Epoch 8\n",
      "train total loss: 0.3806 \n",
      "train accuracy: 0.9318\n",
      "VALIDATION\n",
      "val total loss: 0.5752 \n",
      "val accuracy: 0.7322\n",
      "Epoch 9\n",
      "train total loss: 0.3700 \n",
      "train accuracy: 0.9430\n",
      "VALIDATION\n",
      "val total loss: 0.5668 \n",
      "val accuracy: 0.7322\n",
      "Epoch 10\n",
      "train total loss: 0.3616 \n",
      "train accuracy: 0.9514\n",
      "VALIDATION\n",
      "val total loss: 0.5600 \n",
      "val accuracy: 0.7432\n",
      "Epoch 11\n",
      "train total loss: 0.3555 \n",
      "train accuracy: 0.9578\n",
      "VALIDATION\n",
      "val total loss: 0.5603 \n",
      "val accuracy: 0.7422\n",
      "Epoch 12\n",
      "train total loss: 0.3473 \n",
      "train accuracy: 0.9667\n",
      "VALIDATION\n",
      "val total loss: 0.5451 \n",
      "val accuracy: 0.7583\n",
      "saving with loss of 0.545068196212516 improved over previous 0.5522147106789538\n",
      "Epoch 13\n",
      "train total loss: 0.3427 \n",
      "train accuracy: 0.9708\n",
      "VALIDATION\n",
      "val total loss: 0.5478 \n",
      "val accuracy: 0.7613\n",
      "Epoch 14\n",
      "train total loss: 0.3407 \n",
      "train accuracy: 0.9729\n",
      "VALIDATION\n",
      "val total loss: 0.5457 \n",
      "val accuracy: 0.7633\n",
      "Epoch 15\n",
      "train total loss: 0.3387 \n",
      "train accuracy: 0.9746\n",
      "VALIDATION\n",
      "val total loss: 0.5402 \n",
      "val accuracy: 0.7663\n",
      "saving with loss of 0.5401988688299625 improved over previous 0.545068196212516\n",
      "Epoch 16\n",
      "train total loss: 0.3376 \n",
      "train accuracy: 0.9758\n",
      "VALIDATION\n",
      "val total loss: 0.5477 \n",
      "val accuracy: 0.7523\n",
      "Epoch 17\n",
      "train total loss: 0.3371 \n",
      "train accuracy: 0.9762\n",
      "VALIDATION\n",
      "val total loss: 0.5440 \n",
      "val accuracy: 0.7623\n",
      "Epoch 18\n",
      "train total loss: 0.3368 \n",
      "train accuracy: 0.9765\n",
      "VALIDATION\n",
      "val total loss: 0.5452 \n",
      "val accuracy: 0.7613\n",
      "Epoch 19\n",
      "train total loss: 0.3366 \n",
      "train accuracy: 0.9767\n",
      "VALIDATION\n",
      "val total loss: 0.5454 \n",
      "val accuracy: 0.7603\n",
      "Training complete in 32m 59s\n"
     ]
    }
   ],
   "source": [
    "model_ft1 = train(sense_model, criterion, optim, scheduler, device, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_test = torch.tensor(xytest.invalid_sent)\n",
    "input_ids_test, attention_masks_test = get_roberta_inputs(xytest)\n",
    "test_lists = [input_ids_test, attention_masks_test, invalid_test]\n",
    "test_dataset = text_dataset(x_y_list = test_lists)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(model, data_loader):\n",
    "    results = []\n",
    "    for ipt, maskt, targett in data_loader:\n",
    "        hidden1 = model.init_hidden(ipt.size(0))\n",
    "        hidden2 = model.init_hidden(ipt.size(0))\n",
    "        ipt = ipt.to(device)\n",
    "        maskt = maskt.to(device)\n",
    "        bert_outputs = bert(ipt.reshape((ipt.size(0) * 2, max_seq_length)), attention_mask=maskt.reshape((ipt.size(0) * 2, max_seq_length)))\n",
    "        embeddings = bert_outputs[0].permute(1, 0, 2)\n",
    "        emb1 = embeddings[:, ::2, :]\n",
    "        emb2 = embeddings[:, 1::2, :]\n",
    "        logits, h1, h2 = model(emb1, emb2, hidden1, hidden2)\n",
    "        for i in range(len(logits)):\n",
    "            predicted_index = torch.argmax(logits[i]).item()\n",
    "            results.append(predicted_index)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_results(sense_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.5854%\r\n"
     ]
    }
   ],
   "source": [
    "pd.concat([x_test.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)\n",
    "!python3 eval/taskA_scorer.py --gold-labels data/trial/taskA_trial_answer.csv --pred-labels predictions.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning RoBERTa using all token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "def get_roberta_inputs(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i, row in df.iterrows():\n",
    "        tokendict = tokenizer.encode_plus(row['sent0'], row['sent1'], max_length=50, pad_to_max_length=True)\n",
    "        input_ids.append(tokendict['input_ids'])\n",
    "        attention_masks.append(tokendict['attention_mask'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks = get_roberta_inputs(xy)\n",
    "input_ids_dev, attention_masks_dev = get_roberta_inputs(xydev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "max_seq_length = 50\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list):\n",
    "        self.x_y_list = x_y_list\n",
    "        \n",
    "    def __getitem__(self,index):           \n",
    "        input_ids = self.x_y_list[0][index]\n",
    "        attention_mask = self.x_y_list[1][index]\n",
    "        label = self.x_y_list[2][index]\n",
    "        return input_ids, attention_mask, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "batch_size = 16\n",
    "invalid = torch.tensor(xy.invalid_sent)\n",
    "invalid_dev = torch.tensor(xydev.invalid_sent)\n",
    "train_lists = [input_ids, attention_masks, invalid]\n",
    "val_lists = [input_ids_dev, attention_masks_dev, invalid_dev]\n",
    "\n",
    "training_dataset = text_dataset(x_y_list = train_lists)\n",
    "val_dataset = text_dataset(x_y_list = val_lists)\n",
    "\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val':torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(val_lists[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/ec2-user/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /home/ec2-user/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import RobertaModel\n",
    "class MyRoBertaForSequenceClassification(nn.Module):\n",
    "  \n",
    "    def __init__(self, num_labels=2):\n",
    "        super(MyRoBertaForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
    "        self.classifier = nn.Linear(38400, num_labels)\n",
    "        #nn.init.xavier_normal_(self.classifier.weight)\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        pooled_output = self.bert(input_ids, attention_mask)\n",
    "        #embeddings = pooled_output[2][-1] + pooled_output[2][-2] + pooled_output[2][-3] + pooled_output[2][-4]\n",
    "        embeddings = pooled_output[0].reshape(input_ids.size(0), max_seq_length * 768) #16 x 50 x 768\n",
    "        #embeddings = pooled_output[0][:, 0, :] #16 x 784\n",
    "        #embeddings = embeddings.reshape((input_ids.size(0), max_seq_length * 768))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyRoBertaForSequenceClassification().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, device, num_epochs=25, phases=['train', 'val']):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                print(\"VALIDATION\")\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            corrects = 0\n",
    "                        \n",
    "            # Iterate over data.\n",
    "            for inputs, mask, target in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                mask = mask.to(device)\n",
    "                target = target.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, attention_mask=mask)\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    outputs = F.softmax(outputs,dim=1)  \n",
    "                    loss = criterion(outputs, target)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #print('Running loss : {:.4f}'.format(running_loss))\n",
    "                corrects += torch.sum(torch.max(outputs, 1)[1] == target)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            acc = corrects.double() / dataset_sizes[phase]\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} accuracy: {:.4f}'.format(\n",
    "                phase, acc))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch import optim\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 3\n",
    "lrlast = .001\n",
    "lrmain = 2e-5\n",
    "optim = AdamW(model.parameters(), lr=lrmain, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = (len(train_lists[0]) / batch_size) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/2\n",
      "----------\n",
      "train total loss: 0.5358 \n",
      "train accuracy: 0.7481\n",
      "VALIDATION\n",
      "val total loss: 0.4396 \n",
      "val accuracy: 0.8696\n",
      "saving with loss of 0.43964810410856364 improved over previous 100\n",
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-957d15fa4914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ft1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-fddc3bd39330>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, device, num_epochs, phases)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft1 = train_model(model, criterion, optim, scheduler, device, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_test = torch.tensor(xytest.invalid_sent)\n",
    "input_ids_test, attention_masks_test = get_roberta_inputs(xytest)\n",
    "test_lists = [input_ids_test, attention_masks_test, invalid_test]\n",
    "test_dataset = text_dataset(x_y_list = test_lists)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(data_loader):\n",
    "    results = []\n",
    "    for ipt, maskt, targett in data_loader:\n",
    "        ipt = ipt.to(device)\n",
    "        maskt = maskt.to(device)\n",
    "        outputs = model_ft1(ipt, maskt)\n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        for i in range(len(outputs)):\n",
    "            predicted_index = torch.argmax(outputs[i]).item()\n",
    "            results.append(predicted_index)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_results(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.9659%\r\n"
     ]
    }
   ],
   "source": [
    "pd.concat([x_test.id, pd.Series(results)], axis=1).to_csv('./predictions.csv', header=False, index=False)\n",
    "!python3 eval/taskA_scorer.py --gold-labels data/trial/taskA_trial_answer.csv --pred-labels predictions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval = pd.read_csv('data/test/subtaskA_test_data.csv')\n",
    "input_ids_eval, attention_masks_eval = get_roberta_inputs(x_eval)\n",
    "eval_lists = [input_ids_eval, attention_masks_eval, invalid]\n",
    "eval_dataset = text_dataset(x_y_list = eval_lists)\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size, num_workers=0)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_results(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([x_eval.id, pd.Series(results)], axis=1).to_csv('./subtaskA_answers.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CommonSense.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00492598dbd542c8a84592a5ac065991": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b56c4372bcb459ea8418eeabc92d7fc",
       "IPY_MODEL_58ecfe855eef4271912a91187990e6fb"
      ],
      "layout": "IPY_MODEL_35cdabdbef024a6b81331ea10c614365"
     }
    },
    "03d93305b3754704ac24e5d960c6c5e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "049d01a118444e5aaf684d467c0d07a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c13faf3c61d4e739b43d3b2e2b0e4a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a7f9ba7b72ca4e9fbfd1995f9f046cf7",
       "IPY_MODEL_a564e74ce99b414bb2b3a288370abae0"
      ],
      "layout": "IPY_MODEL_1e07d8379d064f7c8fef772afe0a7ac2"
     }
    },
    "197df6e7f7724935994042f0dec842a4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e07d8379d064f7c8fef772afe0a7ac2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26c778e84160499eaff2f073b05eec3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2cde83d55edf427fa2fbf3e535beac09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4903ddedfef04516b44f534517d07851",
       "IPY_MODEL_46f014b39fad4d0f9dfdaec07c10df3e"
      ],
      "layout": "IPY_MODEL_197df6e7f7724935994042f0dec842a4"
     }
    },
    "35cdabdbef024a6b81331ea10c614365": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46f014b39fad4d0f9dfdaec07c10df3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03d93305b3754704ac24e5d960c6c5e2",
      "placeholder": "",
      "style": "IPY_MODEL_f1ac8a5b3f7e420b97d76fbd536b435b",
      "value": "100% 232k/232k [00:00&lt;00:00, 3.94MB/s]"
     }
    },
    "4903ddedfef04516b44f534517d07851": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_584bead4df0943cbbd1663243d2bd679",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef49a5d738ba4cbe8c5996a099f460ed",
      "value": 231508
     }
    },
    "4c5f563401e840da9afd13fea555d516": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "584bead4df0943cbbd1663243d2bd679": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58ecfe855eef4271912a91187990e6fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_663b3f4172b94ca98738f24923b0fc02",
      "placeholder": "",
      "style": "IPY_MODEL_f85cef55a5724c5ab574f1c09f770d82",
      "value": "100% 440M/440M [00:37&lt;00:00, 11.8MB/s]"
     }
    },
    "5b56c4372bcb459ea8418eeabc92d7fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_931405db76534b1db2a3a82103385cbf",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26c778e84160499eaff2f073b05eec3b",
      "value": 440473133
     }
    },
    "663b3f4172b94ca98738f24923b0fc02": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67b267518b1a46b4ad81f50797c1c6e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "931405db76534b1db2a3a82103385cbf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a564e74ce99b414bb2b3a288370abae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c5f563401e840da9afd13fea555d516",
      "placeholder": "",
      "style": "IPY_MODEL_049d01a118444e5aaf684d467c0d07a6",
      "value": "100% 361/361 [00:00&lt;00:00, 10.6kB/s]"
     }
    },
    "a7f9ba7b72ca4e9fbfd1995f9f046cf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67b267518b1a46b4ad81f50797c1c6e1",
      "max": 361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b263c5db98604c4195acaddc8815fa88",
      "value": 361
     }
    },
    "b263c5db98604c4195acaddc8815fa88": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ef49a5d738ba4cbe8c5996a099f460ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f1ac8a5b3f7e420b97d76fbd536b435b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f85cef55a5724c5ab574f1c09f770d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
